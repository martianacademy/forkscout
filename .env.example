# Forkscout Environment Configuration

# LLM Provider (openai, ollama, anthropic, openrouter)
LLM_PROVIDER=openrouter

# Model name
LLM_MODEL=x-ai/grok-4.1-fast

# API Base URL
LLM_BASE_URL=https://openrouter.ai/api/v1

# For running inside Docker with host Ollama:
# LLM_BASE_URL=http://host.docker.internal:11434/v1

# API Key (only needed for OpenAI/Anthropic)
LLM_API_KEY=

# Temperature (0-1, higher = more creative)
LLM_TEMPERATURE=0.7

# Max tokens per response
LLM_MAX_TOKENS=2000

# SearXNG URL for web search tool
SEARXNG_URL=http://localhost:8888

# Agent configuration
AGENT_MAX_ITERATIONS=10
AGENT_AUTO_REGISTER_TOOLS=true

ADMIN_SECRET=

# Telegram Bot (optional â€” set to enable Telegram bridge)
TELEGRAM_BOT_TOKEN=
