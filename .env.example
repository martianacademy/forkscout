# Forkscout Environment Configuration

# LLM Provider (openai, ollama, anthropic, openrouter)
LLM_PROVIDER=openrouter

# Model name
LLM_MODEL=x-ai/grok-4.1-fast

# API Base URL
LLM_BASE_URL=https://openrouter.ai/api/v1

# For running inside Docker with host Ollama:
# LLM_BASE_URL=http://host.docker.internal:11434/v1

# API Key (only needed for OpenAI/Anthropic)
LLM_API_KEY=

# Temperature (0-1, higher = more creative)
LLM_TEMPERATURE=0.7

# Max tokens per response
LLM_MAX_TOKENS=2000

# SearXNG URL for web search tool
SEARXNG_URL=http://localhost:8888

# Agent configuration
AGENT_MAX_ITERATIONS=10
AGENT_AUTO_REGISTER_TOOLS=true

ADMIN_SECRET=

# Telegram Bot (optional — set to enable Telegram bridge)
TELEGRAM_BOT_TOKEN=

MODEL_FAST=google/gemini-2.0-flash-001
MODEL_BALANCED=x-ai/grok-4.1-fast
MODEL_POWERFUL=anthropic/claude-sonnet-4
BUDGET_DAILY_USD=5
BUDGET_MONTHLY_USD=50

# Per-tier provider overrides (optional — defaults to MODEL_PROVIDER / LLM_PROVIDER)
# Supported providers: openrouter, openai, anthropic, google, ollama, openai-compatible
# MODEL_FAST_PROVIDER=google
# MODEL_FAST_API_KEY=AIza...
# MODEL_FAST_BASE_URL=
# MODEL_BALANCED_PROVIDER=openrouter
# MODEL_BALANCED_API_KEY=
# MODEL_POWERFUL_PROVIDER=anthropic
# MODEL_POWERFUL_API_KEY=sk-ant-...
